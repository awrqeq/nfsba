# /home/machao/pythonproject/nfsba/utils/losses.py
import torch
import torch.nn.functional as F
import math

from constants import CIFAR10_MEAN, CIFAR10_STD
from .dct import block_idct, get_freq_bands_indices # 导入频带索引函数
from .quantization import simulate_jpeg_quant
from .stats import calculate_stat_loss

# --- 原 perturbation_loss 保留 ---
def perturbation_loss(delta_f_flat):
    """Calculates the L2 norm squared loss for the perturbation."""
    return F.mse_loss(delta_f_flat, torch.zeros_like(delta_f_flat))

# --- 原 statistics_loss 保留 ---
def statistics_loss(dct_blocks_poisoned, target_stats, fixed_beta=1.0):
    """
    Wrapper for calculating distribution and energy statistical losses.
    Returns:
        loss_dist (tensor): Distribution parameter matching loss.
        loss_energy (tensor): Energy ratio matching loss.
    """
    return calculate_stat_loss(dct_blocks_poisoned, target_stats, fixed_beta=fixed_beta)

# --- 新增: 特征一致性损失 ---
def feature_consistency_loss(features_poisoned, target_feature_center, loss_type='l2'):
    """
    Computes the loss between poisoned features and the target feature center.
    Args:
        features_poisoned (Tensor): Features extracted from poisoned images (BatchSize, FeatureDim).
        target_feature_center (Tensor): Precomputed average feature for the target class (1, FeatureDim).
        loss_type (str): 'l2' for Mean Squared Error or 'cosine' for Cosine Embedding Loss.
    Returns:
        loss (Tensor): Scalar loss value.
    """
    if features_poisoned is None or features_poisoned.numel() == 0:
        return torch.tensor(0.0, device=target_feature_center.device)

    # Ensure target_center is on the same device and expanded
    target_center_expanded = target_feature_center.to(features_poisoned.device).expand_as(features_poisoned)

    if loss_type == 'l2':
        loss = F.mse_loss(features_poisoned, target_center_expanded)
    elif loss_type == 'cosine':
        # CosineEmbeddingLoss expects targets of 1 (for similarity)
        target_similarity = torch.ones(features_poisoned.size(0), device=features_poisoned.device)
        loss = F.cosine_embedding_loss(features_poisoned, target_center_expanded, target_similarity)
    else:
        raise ValueError(f"Unknown loss_type for feature consistency: {loss_type}")

    return loss

# --- 新增: 频域平滑度损失 ---
def frequency_smoothness_loss(perturbation_dct_blocks, block_size=8, high_freq_threshold=30):
    """
    Computes the L1 norm of the high-frequency components of the DCT perturbation.
    Args:
        perturbation_dct_blocks (Tensor): The perturbation generated by the generator
                                          in the DCT domain (B, C, Bh, Bw, H_block, W_block).
                                          Shape might also be (N, 1, H_block, W_block) for CNN gen output.
        block_size (int): Size of the DCT block (e.g., 8).
        high_freq_threshold (int): Zigzag index threshold (exclusive) to define high frequencies.
                                    Indices >= high_freq_threshold are considered high.
    Returns:
        loss (Tensor): Scalar loss value (L1 norm of high-freq components).
    """
    if perturbation_dct_blocks is None or perturbation_dct_blocks.numel() == 0:
        return torch.tensor(0.0, device=perturbation_dct_blocks.device)

    # 获取高频系数的扁平化索引
    try:
        # 假设低频阈值不重要，中频阈值定义了高频的开始
        _, _, mid_indices, high_indices = get_freq_bands_indices(block_size, low_thresh=high_freq_threshold // 2, mid_thresh=high_freq_threshold)
        # 我们只关心高频 high_indices
        if not high_indices:
            print(f"Warning: No high frequency indices found with threshold {high_freq_threshold} for block size {block_size}.")
            return torch.tensor(0.0, device=perturbation_dct_blocks.device)
    except Exception as e:
        print(f"Error getting frequency band indices: {e}. Skipping smoothness loss.")
        return torch.tensor(0.0, device=perturbation_dct_blocks.device)

    # 将扰动块展平以便索引
    # 输入可能是 (B, C, Bh, Bw, H, W) 或 (N, 1, H, W)
    original_shape = perturbation_dct_blocks.shape
    if len(original_shape) == 6: # (B, C, Bh, Bw, H, W)
        perturb_flat = perturbation_dct_blocks.reshape(-1, block_size * block_size)
    elif len(original_shape) == 4 and original_shape[1] == 1: # (N, 1, H, W) for CNN output
        perturb_flat = perturbation_dct_blocks.reshape(-1, block_size * block_size)
    else:
        print(f"Warning: Unexpected perturbation shape {original_shape} for smoothness loss.")
        return torch.tensor(0.0, device=perturbation_dct_blocks.device)


    # 提取高频系数并计算 L1 范数
    high_freq_coeffs = perturb_flat[:, high_indices]
    loss = torch.mean(torch.abs(high_freq_coeffs)) # 使用 L1 范数均值

    return loss


# --- 移除旧的 compute_trigger_loss ---

# --- 更新 calculate_generator_loss ---
def calculate_generator_loss(losses_dict, weights_dict):
    """
    Calculates the final weighted generator loss using the new structure.
    Args:
        losses_dict (dict): Contains calculated loss values (e.g., {'dist': loss_d, 'energy': loss_e, ...})
        weights_dict (dict): Contains corresponding lambda weights (e.g., {'dist': lambda_d, 'energy': lambda_e, ...})
    Returns:
        total_loss (tensor): The final weighted loss.
    """
    # 确定设备，优先使用 feature_consistency 的设备
    if 'feature_consistency' in losses_dict and torch.is_tensor(losses_dict['feature_consistency']):
        device = losses_dict['feature_consistency'].device
    elif 'smooth_freq' in losses_dict and torch.is_tensor(losses_dict['smooth_freq']):
         device = losses_dict['smooth_freq'].device
    else:
        device = 'cpu' # Fallback
    total_loss = torch.tensor(0.0, device=device)

    # 定义所有可能的损失项及其对应的键
    loss_keys = ['dist', 'energy', 'feature_consistency', 'smooth_freq', 'perturb']

    for key in loss_keys:
        if key in losses_dict and key in weights_dict and weights_dict[key] > 0:
            loss_val = losses_dict[key]
            # 确保损失值有效（非 NaN）再加权累加
            is_invalid = False
            if torch.is_tensor(loss_val):
                is_invalid = torch.isnan(loss_val).item() or torch.isinf(loss_val).item()
            elif isinstance(loss_val, float):
                is_invalid = math.isnan(loss_val) or math.isinf(loss_val)
            else: # 如果不是 tensor 或 float，则认为无效
                is_invalid = True
                print(f"Warning: Loss '{key}' has unexpected type {type(loss_val)}. Skipping.")

            if not is_invalid:
                total_loss += weights_dict[key] * loss_val
            elif weights_dict[key] > 0: # 只有当权重不为0时才打印警告
                print(f"Warning: Loss '{key}' is NaN or Inf. Skipping this term.")

    # 移除了对 'trigger' 和 'lpips' 的处理

    return total_loss